@Article{Liu,
author="Liu, Yugang
and Nejat, Goldie",
title="Robotic Urban Search and Rescue: A Survey from the Control Perspective",
journal="Journal of Intelligent {\&} Robotic Systems",
year="2013",
month="Nov",
day="01",
volume="72",
number="2",
pages="147--165",
abstract="Robotic urban search and rescue (USAR) is a challenging yet promising research area which has significant application potentials as has been seen during the rescue and recovery operations of recent disaster events. To date, the majority of rescue robots used in the field are teleoperated. In order to minimize a robot operator's workload in time-critical disaster scenes, recent efforts have been made to equip these robots with some level of autonomy. This paper provides a detailed overview of developments in the exciting and challenging area of robotic control for USAR environments. In particular, we discuss the efforts that have been made in the literature towards: 1) developing low-level controllers for rescue robot autonomy in traversing uneven terrain and stairs, and perception-based simultaneous localization and mapping (SLAM) algorithms for developing 3D maps of USAR scenes, 2) task sharing of multiple tasks between operator and robot via semi-autonomous control, and 3) high-level control schemes that have been designed for multi-robot rescue teams.",
issn="1573-0409",
doi="10.1007/s10846-013-9822-x",
url="https://doi.org/10.1007/s10846-013-9822-x"
}

@ARTICLE{Murphy,
author={R. R. {Murphy}},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
title={Human-robot interaction in rescue robotics},
year={2004},
volume={34},
number={2},
pages={138-153},
keywords={human computer interaction;telerobotics;mobile robots;intelligent robots;artificial intelligence;human factors;user modelling;rescue robotics;human-robot interaction;domain theory;artificial intelligence;human factors;intelligent robots;mobile robots;robots sensing systems;telerobotics;user modeling;Human robot interaction;Cognitive robotics;Robot sensing systems;Intelligent robots;Mobile robots;Teamwork;Robot control;Communication system control;Mobile communication;Human factors},
doi={10.1109/TSMCC.2004.826267},
ISSN={1094-6977},
month={May},}


@Article{Mortimer,
AUTHOR = {Mortimer, Michael and Horan, Ben and Seyedmahmoudian, Mehdi},
TITLE = {Building a Relationship between Robot Characteristics and Teleoperation User Interfaces},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {3},
ARTICLE-NUMBER = {587},
URL = {http://www.mdpi.com/1424-8220/17/3/587},
ISSN = {1424-8220},
ABSTRACT = {The Robot Operating System (ROS) provides roboticists with a standardized and distributed framework for real-time communication between robotic systems using a microkernel environment. This paper looks at how ROS metadata, Unified Robot Description Format (URDF), Semantic Robot Description Format (SRDF), and its message description language, can be used to identify key robot characteristics to inform User Interface (UI) design for the teleoperation of heterogeneous robot teams. Logical relationships between UI components and robot characteristics are defined by a set of relationship rules created using relevant and available information including developer expertise and ROS metadata. This provides a significant opportunity to move towards a rule-driven approach for generating the designs of teleoperation UIs; in particular the reduction of the number of different UI configurations required to teleoperate each individual robot within a heterogeneous robot team. This approach is based on using an underlying rule set identifying robots that can be teleoperated using the same UI configuration due to having the same or similar robot characteristics. Aside from reducing the number of different UI configurations an operator needs to be familiar with, this approach also supports consistency in UI configurations when a teleoperator is periodically switching between different robots. To achieve this aim, a Matlab toolbox is developed providing users with the ability to define rules specifying the relationship between robot characteristics and UI components. Once rules are defined, selections that best describe the characteristics of the robot type within a particular heterogeneous robot team can be made. A main advantage of this approach is that rather than specifying discrete robots comprising the team, the user can specify characteristics of the team more generally allowing the system to deal with slight variations that may occur in the future. In fact, by using the defined relationship rules and characteristic selections, the toolbox can automatically identify a reduced set of UI configurations required to control possible robot team configurations, as opposed to the traditional ad-hoc approach to teleoperation UI design. In the results section, three test cases are presented to demonstrate how the selection of different robot characteristics builds a number of robot characteristic combinations, and how the relationship rules are used to determine a reduced set of required UI configurations needed to control each individual robot in the robot team.},
DOI = {10.3390/s17030587}
}

@article{Belter,
author = {Dominik Belter and Piotr Skrzypczynski},
title = {Precise self‐localization of a walking robot on rough terrain using parallel tracking and mapping},
journal = {Industrial Robot: the international journal of robotics research and application},
volume = {40},
number = {3},
pages = {229-237},
year = {2013},
doi = {10.1108/01439911311309924},

URL = { 
        https://doi.org/10.1108/01439911311309924
    
},
eprint = { 
        https://doi.org/10.1108/01439911311309924
    
}
,
    abstract = { Purpose – The purpose of this paper is to describe a novel application of the recently introduced concept from computer vision to self‐localization of a walking robot in unstructured environments. The technique described in this paper enables a walking robot with a monocular vision system (single camera) to obtain precise estimates of its pose with regard to the six degrees of freedom. This capability is essential in search and rescue missions in collapsed buildings, polluted industrial plants, etc.Design/methodology/approach – The Parallel Tracking and Mapping (PTAM) algorithm and the Inertial Measurement Unit (IMU) are used to determine the 6‐d.o.f. pose of a walking robot. Bundle‐adjustment‐based tracking and structure reconstruction are applied to obtain precise camera poses from the monocular vision data. The inclination of the robot's platform is determined by using IMU. The self‐localization system is used together with the RRT‐based motion planner, which allows to walk autonomously on rough, previously unknown terrain. The presented system operates on‐line on the real hexapod robot. Efficiency and precision of the proposed solution are demonstrated by experimental data.Findings – The PTAM‐based self‐localization system enables the robot to walk autonomously on rough terrain. The software operates on‐line and can be implemented on the robot's on‐board PC. Results of the experiments show that the position error is small enough to allow robust elevation mapping using the laser scanner. In spite of the unavoidable feet slippages, the walking robot which uses PTAM for self‐localization can precisely estimate its position and successfully recover from motion execution errors.Research limitations/implications – So far the presented self‐localization system was tested in limited‐scale indoor experiments. Experiments with more realistic outdoor scenarios are scheduled as further work.Practical implications – Precise self‐localization may be one of the most important factors enabling the use of walking robots in practical USAR missions. The results of research on precise self‐localization in 6‐d.o.f. may be also useful for autonomous robots in other application areas: construction, agriculture, military.Originality/value – The vision‐based self‐localization algorithm used in the presented research is not new, but the contribution lies in its implementation/integration on a walking robot, and experimental evaluation in the demanding problem of precise self‐localization in rough terrain. }
}

@Article{Santos,
author="Santos, Jo{\~a}o Machado
and Couceiro, Micael S.
and Portugal, David
and Rocha, Rui P.",
title="A Sensor Fusion Layer to Cope with Reduced Visibility in {SLAM}",
journal="Journal of Intelligent {\&} Robotic Systems",
year="2015",
month="Dec",
day="01",
volume="80",
number="3",
pages="401--422",
abstract="Mapping and navigating with mobile robots in scenarios with reduced visibility, e.g. due to smoke, dust, or fog, is still a big challenge nowadays. In spite of the tremendous advance on Simultaneous Localization and Mapping (SLAM) techniques for the past decade, most of current algorithms fail in those environments because they usually rely on optical sensors providing dense range data, e.g. laser range finders, stereo vision, LIDARs, RGB-D, etc., whose measurement process is highly disturbed by particles of smoke, dust, or steam. This article addresses the problem of performing SLAM under reduced visibility conditions by proposing a sensor fusion layer which takes advantage from complementary characteristics between a laser range finder (LRF) and an array of sonars. This sensor fusion layer is ultimately used with a state-of-the-art SLAM technique to be resilient in scenarios where visibility cannot be assumed at all times. Special attention is given to mapping using commercial off-the-shelf (COTS) sensors, namely arrays of sonars which, being usually available in robotic platforms, raise technical issues that were investigated in the course of this work. Two sensor fusion methods, a heuristic method and a fuzzy logic-based method, are presented and discussed, corresponding to different stages of the research work conducted. The experimental validation of both methods with two different mobile robot platforms in smoky indoor scenarios showed that they provide a robust solution, using only COTS sensors, for adequately coping with reduced visibility in the SLAM process, thus decreasing significantly its impact in the mapping and localization results obtained.",
issn="1573-0409",
doi="10.1007/s10846-015-0180-8",
url="https://doi.org/10.1007/s10846-015-0180-8"
}

@ARTICLE{Kougianos,
author={E. {Kougianos} and S. P. {Mohanty} and G. {Coelho} and U. {Albalawi} and P. {Sundaravadivel}},
journal={IEEE Access},
title={Design of a High-Performance System for Secure Image Communication in the Internet of Things},
year={2016},
volume={4},
number={},
pages={1222-1242},
keywords={aerospace computing;control engineering computing;data compression;helicopters;image coding;image sensors;Internet of Things;object tracking;robot vision;telecommunication security;three-term control;variable structure systems;high-performance system design;secure image communication;Internet-of-things;image exchange;video exchange;smart health care;smart structures;smart transportations;IoT;modular quadrotor architecture;automatic tracking applications;off-the-shelf components;system prototyping;target tracking application;proportional-integral-derivative control;custom target acquisition algorithm;sliding-window-based algorithm;search-and-rescue;automatic object tracking;traffic congestion analysis;better portable graphics compression algorithm;hardware architecture;secure digital camera;SDC;secure BPG compression algorithm;SBPG compression algorithm;Simulink;Internet of Things;Object detection;Image compression;Video communication;Image coding;Algorithm design and analysis;Target tracking;Compression algorithms;Transform coding;Smart devices;Internet of Things (IoT);Object Detection;Robot Vision Systems;Secure Digital Camera (SDC);Better Portable Graphics (BPG);Image Compression;VLSI Architecture;Internet of things (IoT);object detection;robot vision systems;secure digital camera (SDC);better portable graphics (BPG);image compression;VLSI architecture},
doi={10.1109/ACCESS.2016.2542800},
ISSN={2169-3536},
month={},}

@ARTICLE{Boukas,
author={E. {Boukas} and I. {Kostavelis} and A. {Gasteratos} and G. C. {Sirakoulis}},
journal={IEEE Transactions on Automation Science and Engineering},
title={Robot Guided Crowd Evacuation},
year={2015},
volume={12},
number={2},
pages={739-751},
keywords={cellular automata;emergency management;mobile robots;simulation;robot guided evacuation model;custom-made mobile robotic platform;emergency situations;human behavior;cellular automaton simulation model;mobile robot agents;innovative technological solution;crowd evacuation problem;citizens safety;robot guided crowd evacuation;Computational modeling;Mobile communication;Safety;Mobile robots;Robot sensing systems;Automata;Cellular automata (CA);evacuation models;Robot guidance;safety;simultaneously localization and mapping (SLAM)},
doi={10.1109/TASE.2014.2323175},
ISSN={1545-5955},
month={April},}

@article{Tuna,
title = "An autonomous wireless sensor network deployment system using mobile robots for human existence detection in case of disasters",
journal = "Ad Hoc Networks",
volume = "13",
pages = "54 - 68",
year = "2014",
issn = "1570-8705",
doi = "https://doi.org/10.1016/j.adhoc.2012.06.006",
url = "http://www.sciencedirect.com/science/article/pii/S157087051200114X",
author = "Gurkan Tuna and V. Cagri Gungor and Kayhan Gulez",
keywords = "Wireless sensor networks, Human existence detection, Cooperative Simultaneous Localization and Mapping (SLAM), Map merging, Sensor node deployment strategies",
abstract = "This paper presents a novel approach of using autonomous mobile robots to deploy a Wireless Sensor Network (WSN) for human existence detection in case of disasters. During WSN deployment, mobile robots perform cooperative Simultaneous Localization and Mapping (SLAM) and communicate over the WSN. The proposed system has important advantages over a human-assisted system, including autonomous deployment, aggregated intelligence, and flexibility. However, the realization of these envisaged gains depends on communication and coordination capabilities of the system. In this study, the advantages of an autonomous WSN deployment system by mobile robots, design principles and implementation related issues have been explained. In addition, simulation studies have been performed to show the effectiveness of the proposed approach considering WSN coverage, coordination strategies, and SLAM perspectives. Overall, this paper addresses the advantages of using multiple robots for WSN deployment in terms of cooperative exploration and cooperative SLAM, the benefit of simultaneously deploying wireless sensor nodes during the exploration of an unknown deployment zone and the use of WSN-based communication as an alternative communication method during exploration."
}
@INPROCEEDINGS{Fu,
author={ Siyao Fu
and Zeng-Guang Zhou
and Guosheng Yang },
booktitle={2009 International Conference on Networking, Sensing and Control},
title={An indoor navigation system for autonomous mobile robot using wireless sensor network},
year={2009},
volume={},
number={},
pages={227-232},
keywords={mobile robots;path planning;wireless sensor networks;indoor navigation system;autonomous mobile robot;wireless sensor network;distributed monitoring tool;composite net system;robot pose;RF network;Mobile robots;Wireless sensor networks;Robot sensing systems;Sensor systems;Intelligent sensors;Robot kinematics;Indoor environments;Radio navigation;Wireless communication;Pervasive computing},
doi={10.1109/ICNSC.2009.4919277},
ISSN={},
month={March},}

@article{Barber,
author = {Ramon Barber and Miguel A. Rodriguez-Conejo and Juan Melendez and Santiago Garrido},
title ={Design of an Infrared Imaging System for Robotic Inspection of Gas Leaks in Industrial Environments},
journal = {International Journal of Advanced Robotic Systems},
volume = {12},
number = {3},
pages = {23},
year = {2015},
doi = {10.5772/60058},

URL = { 
        https://doi.org/10.5772/60058
    
},
eprint = { 
        https://doi.org/10.5772/60058
    
}
,
    abstract = { Gas detection can become a critical task in dangerous environments that involve hazardous or contaminant gases, and the use of imaging sensors provides an important tool for leakage location. This paper presents a new design for remote sensing of gas leaks based on infrared (IR) imaging techniques. The inspection system uses an uncooled microbolometer detector, operating over a wide spectral bandwidth, that features both low size and low power consumption. This equipment is boarded on a robotic platform, so that wide objects or areas can be scanned.The detection principle is based on the use of active imaging techniques, where the use of external IR illumination enhances the detection limit and allows the proposed system to operate in most cases independently from environmental conditions, unlike passive commercial approaches. To illustrate this concept, a fully radiometric description of the detection problem has been developed; CO2 detection has been demonstrated; and simulations of typical gas detection scenarios have been performed, showing that typical industrial leaks of CH4 are well within the detection limits.The mobile platform where the gas sensing system is going to be implemented is a robot called TurtleBot. The control of the mobile base and of the inspection device is integrated in ROS architecture. The exploration system is based on the technique of Simultaneous Localization and Mapping (SLAM) that makes it possible to locate the gas leak in the map. }
}

@ARTICLE{Cadena,
author={C. {Cadena} and L. {Carlone} and H. {Carrillo} and Y. {Latif} and D. {Scaramuzza} and J. {Neira} and I. {Reid} and J. J. {Leonard}},
journal={IEEE Transactions on Robotics},
title={Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age},
year={2016},
volume={32},
number={6},
pages={1309-1332},
keywords={SLAM (robots);simultaneous-localization-and-mapping;SLAM community;long-term mapping;semantic representations;SLAM users;critical eye;Graph theory;Simultaneous location and mapping;Service robots;Robustness;Localization;Factor graphs;localization;mapping;maximum a posteriori estimation;perception;robots;sensing;simultaneous localization and mapping (SLAM)},
doi={10.1109/TRO.2016.2624754},
ISSN={1552-3098},
month={Dec},}

@article{Saeedi,
author = {Saeedi, Sajad and Trentini, Michael and Seto, Mae and Li, Howard},
title = {Multiple-Robot Simultaneous Localization and Mapping: A Review},
journal = {Journal of Field Robotics},
volume = {33},
number = {1},
pages = {3-46},
doi = {10.1002/rob.21620},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21620},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21620},
abstract = {Simultaneous localization and mapping (SLAM) in unknown GPS-denied environments is a major challenge for researchers in the field of mobile robotics. Many solutions for single-robot SLAM exist; however, moving to a platform of multiple robots adds many challenges to the existing problems. This paper reviews state-of-the-art multiple-robot systems, with a major focus on multiple-robot SLAM. Various issues and problems in multiple-robot SLAM are introduced, current solutions for these problems are reviewed, and their advantages and disadvantages are discussed.},
year = {2016}
}

@ARTICLE{Wu,
author={M. {Wu} and S. {Lam} and T. {Srikanthan}},
journal={IEEE Transactions on Intelligent Transportation Systems},
title={A Framework for Fast and Robust Visual Odometry},
year={2017},
volume={18},
number={12},
pages={3433-3448},
keywords={computational complexity;computer vision;distance measurement;driver information systems;feature extraction;image sequences;motion estimation;optimisation;tracking;ego-vehicle motion state;KLT feature detection;adaptive integration window technique;RANSAC algorithm;uncontrolled realistic urban environments;autonomous driving;advanced driver assistance systems;collision risk;robust visual odometry;fast odometry;accurate ego-motion estimation;visual odometry methods;KITTI odometry data;motion estimation process;Gauss-Newton optimization scheme;random sample consensus algorithm;robust runtime efficient way;feature correspondences;conventional KLT tracker;automatic tracking failure detection scheme;smooth motion constraint;Kanade-Lucas-Tomasi;pruning method;core stages;robust techniques;runtime-efficient strategies;low computational complexity;Feature extraction;Tracking;Visualization;Collision avoidance;Motion estimation;Computational complexity;Autonomous vehicles;Visual odometry;ego-motion;collision avoidance;feature detection;feature tracking;motion estimation;ADASs;autonomous vehicles},
doi={10.1109/TITS.2017.2685433},
ISSN={1524-9050},
month={Dec},}

@Inbook{Guimaraes,
author="Guimar{\~a}es, Rodrigo Longhi
and de Oliveira, Andr{\'e} Schneider
and Fabro, Jo{\~a}o Alberto
and Becker, Thiago
and Brenner, Vin{\'i}cius Amilgar",
editor="Koubaa, Anis",
title="ROS Navigation: Concepts and Tutorial",
bookTitle="Robot Operating System (ROS): The Complete Reference (Volume 1)",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="121--160",
abstract="This tutorial chapter aims to teach the main theoretical concepts and explain the use of ROS Navigation Stack. This is a powerful toolbox to path planning and Simultaneous Localization And Mapping (SLAM) but its application is not trivial due to lack of comprehension of the related concepts. This chapter will present the theory inside this stack and explain in an easy way how to perform SLAM in any robot. Step by step guides, example codes explained (line by line) and also real robot testing will be available. We will present the requisites and the how-to's that will make the readers able to set the odometry, establish reference frames and its transformations, configure perception sensors, tune the navigation controllers and plan the path on their own virtual or real robots.",
isbn="978-3-319-26054-9",
doi="10.1007/978-3-319-26054-9_6",
url="https://doi.org/10.1007/978-3-319-26054-9_6"
}

@InProceedings{Czarnetzki,
author="Czarnetzki, Stefan
and Hegele, Maximilian
and Kerner, S{\"o}ren",
editor="Ruiz-del-Solar, Javier
and Chown, Eric
and Pl{\"o}ger, Paul G.",
title="Odometry Correction for Humanoid Robots Using Optical Sensors",
booktitle="RoboCup 2010: Robot Soccer World Cup XIV",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="48--59",
abstract="Odometry measurement is an important concept to update localization information, but is prone to error propagation. Still the method is widely applied to wheeled mobile robots since their motion is quite robust to random error such as slipping. While the concept of odometry can also be applied to humanoid robots the dynamically stable walking generation reinforces sliding motions resulting in unpredictable errors. Therefore this paper proposes a novel approach to measure these sliding errors with the help of optical sensors to either correct the odometry update or perform suitable actions to counteract the error.",
isbn="978-3-642-20217-9"
}


@Article{Kamarudin,
AUTHOR = {Kamarudin, Kamarulzaman and Mamduh, Syed Muhammad and Shakaff, Ali Yeon Md and Zakaria, Ammar},
TITLE = {Performance Analysis of the Microsoft Kinect Sensor for 2D Simultaneous Localization and Mapping (SLAM) Techniques},
JOURNAL = {Sensors},
VOLUME = {14},
YEAR = {2014},
NUMBER = {12},
PAGES = {23365--23387},
URL = {http://www.mdpi.com/1424-8220/14/12/23365},
PubMedID = {25490595},
ISSN = {1424-8220},
ABSTRACT = {This paper presents a performance analysis of two open-source, laser  scanner-based Simultaneous Localization and Mapping (SLAM) techniques (i.e., Gmapping and Hector SLAM) using a Microsoft Kinect to replace the laser sensor. Furthermore, the paper proposes a new system integration approach whereby a Linux virtual machine is used to run the open source SLAM algorithms. The experiments were conducted in two different environments; a small room with no features and a typical office corridor with desks and chairs. Using the data logged from real-time experiments, each SLAM technique was simulated and tested with different parameter settings. The results show that the system is able to achieve real time SLAM operation. The  system implementation offers a simple and reliable way to compare the performance of Windows-based SLAM algorithm with the algorithms typically implemented in a Robot Operating System (ROS). The results also indicate that certain modifications to the default laser scanner-based parameters are able to improve the map accuracy. However, the limited field of view and range of Kinect’s depth sensor often causes the map to be inaccurate, especially in featureless areas, therefore the Kinect sensor is not a direct replacement for a laser scanner, but rather offers a feasible alternative for 2D SLAM tasks.},
DOI = {10.3390/s141223365}
}


